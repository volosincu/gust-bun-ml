# -*- coding: utf-8 -*-
"""Gust_Bun.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TeiDVxUlqhB_xcZaRmo_uzvrjF_Wmelx
"""

import tensorflow as tf

import numpy as np
import pandas as pd
import os
import time


DEBUG = False

def withDebug (a):
  if DEBUG:
    print(a)

class TextGenModel(tf.keras.Model):
  def __init__(self, alfabet_size, embedding_dim, rnn_units):
    super().__init__(self)
    self.embedding = tf.keras.layers.Embedding(alfabet_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(rnn_units,
                                   return_sequences=True,
                                   return_state=True)
    self.dense = tf.keras.layers.Dense(alfabet_size)

  def call(self, inputs, states=None, return_state=False, training=False):
    x = inputs
    x = self.embedding(x, training=training)
    if states is None:
      states = self.gru.get_initial_state(x)
    x, states = self.gru(x, initial_state=states, training=training)
    x = self.dense(x, training=training)

    if return_state:
      return x, states
    else:
      return x


class GradientTraining(TextGenModel):
  @tf.function
  def train_step(self, inputs):
      inputs, labels = inputs
      with tf.GradientTape() as tape:
          predictions = self(inputs, training=True)
          loss = self.loss(labels, predictions)
      grads = tape.gradient(loss, model.trainable_variables)
      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))

      return {'loss': loss}

# Read, then decode for py2 compat.
TEXT_RETETE = open('/rețete-vegetariene.txt', 'rb').read().decode(encoding='utf-8')
# length of TEXT_RETETE is the number of characters in it
withDebug(f'Length of text: {len(TEXT_RETETE)} characters')



# Take a look at the first 250 characters in text
# withDebug("\nPrima rețetă: ")
# withDebug(TEXT_RETETE.split("Rețetă:")[:2])

# The unique characters in the file
alfabet = sorted(set(TEXT_RETETE))
withDebug(f'{len(alfabet)} unique characters')


withDebug(f'{alfabet} alfabet')


example_texts = ['piper', 'sare']
chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')
withDebug(f'{chars} chars test')
ids_from_chars = tf.keras.layers.StringLookup(
    vocabulary=list(alfabet), mask_token=None)

ids = ids_from_chars(chars)


chars_from_ids = tf.keras.layers.StringLookup(
    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)
chars = chars_from_ids(ids)


tf.strings.reduce_join(chars, axis=-1).numpy()


def text_from_ids(ids):
  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)


all_ids = ids_from_chars(tf.strings.unicode_split(TEXT_RETETE, 'UTF-8'))
print(tf.strings.split(TEXT_RETETE, 'UTF-8'))

ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)


for ids in ids_dataset.take(10):
    print(chars_from_ids(ids).numpy())
    withDebug(chars_from_ids(ids).numpy().decode('utf-8'))

seq_length = 100

sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)
for seq in sequences.take(1):
  withDebug(chars_from_ids(seq))

for seq in sequences.take(5):
  withDebug(text_from_ids(seq).numpy())

from collections import Counter
def wordCounter(text):
  count = Counter()
  for _word in text:
    word = None
    if _word.startswith('('):
      word = _word[1:]
    elif _word.endswith(')'):
      word = _word[:len(_word)-1]
    else:
      word = _word
    count[word] += 1
    
  return count

textVect = TEXT_RETETE.split()
counter = wordCounter(textVect)
print(f'Nr de cuvinte din datasetul de antrenament: {len(counter)}')
x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}

Counter(textVect).most_common(2)

"""Se pregateste setul de date pt tokenizare (sub forma de id-uri)"""

textNumpyVect = pd.DataFrame(textVect).to_numpy()
textNumpyVect.shape

txtv = np.squeeze(textNumpyVect, axis=1)
txtv

cuvinteUnice = len(counter)
vocabRetete = cuvinteUnice
tknz = tf.keras.preprocessing.text.Tokenizer(num_words=cuvinteUnice)
tknz.fit_on_texts(txtv)

wordIndex = tknz.word_index
# wordIndex

# acelasi text dar in loc de cuvinte avem id-uri
wordSequences = tknz.texts_to_sequences(txtv)

print(txtv[230:240])
print(wordSequences[230:240])

# exemple de cuvinte compuse: 1/4, 1-2

maxPad = 0
for seq in wordSequences:
  # if 2 == len(seq):
  #   print(f'{seq}')
  if maxPad < len(seq):
    maxPad = len(seq)

print(maxPad)

"""Pad pentru """

wordSequencesPadded = tf.keras.preprocessing.sequence.pad_sequences(wordSequences, maxlen=maxPad, padding='post', truncating='post')
wordSequencesPadded.shape

wordSequencesPadded[:3]

reverseWordIndex = dict([(id, word) for (word, id) in wordIndex.items()])
#reverseWordIndex

def decodeIds(seq):
  return " ".join([reverseWordIndex.get(id, "") for id in seq])

for sq in wordSequencesPadded[:3]:
  print(decodeIds(sq))

"""# When to use a Sequential model
https://www.tensorflow.org/guide/keras/sequential_model
"""

# model = tf.keras.models.Sequential()
# model.add(tf.keras.layers.Embedding(vocabRetete, 2))

# model.add(tf.keras.layers.LSTM(64, dropout=0.1))
# model.add(tf.keras.layers.Dense(2, activation='relu'))
# model.add(tf.keras.layers.Dense(1))

# model.summary()

# loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)
# #loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
# optim = tf.keras.optimizers.Adam(lr=0.001)
# model.compile(loss=loss, optimizer=optim, metrics=['accuracy'])




# model.fit(dataset, epochs=1)

ids_dataset = tf.data.Dataset.from_tensor_slices(wordSequencesPadded)
ids_dataset

sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)

# for sq in sequences.take(1):
#   print(sq)
#   for s in sq[0]:
#     print(s)


def split_input_target(sequence):
    input_text = sequence[:-1]
    target_text = sequence[1:]
    return input_text, target_text


dataset = sequences.map(split_input_target)

EPOCHS = 1
model = GradientTraining(
    alfabet_size=vocabRetete,
    embedding_dim=5,
    rnn_units=1024)

model.compile(optimizer = tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

model.fit(dataset, epochs=EPOCHS)

example_batch_predictions = model(wordSequencesPadded[:1])
# print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")
# print(tf.squeeze(example_batch_predictions))

preds = tf.reshape(example_batch_predictions, [197*2])
print(preds[:10])

# temperature = 1.0

# # predicted_logits = predicted_logits[:, -1]


# print(predicted_logits)

# predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)

# print(predicted_ids)
# predicted_logits = predicted_logits/temperature
# # Apply the prediction mask: prevent "[UNK]" from being generated.
# # predicted_logits = predicted_logits + prediction_mask

# # Sample the output logits to generate token IDs.
# predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)
# predicted_ids = tf.squeeze(predicted_ids, axis=-1)

def model_gen_reteta(wordId):
    example_batch_predictions = model(wordId)
    predics = example_batch_predictions[0]
    # print(predics.shape)
    sampled_indices = tf.random.categorical(predics, num_samples=1)
    sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()
    return sampled_indices

# wordIndex

wordSequencesPadded[:1]

np.reshape([89, 0], (1, 2))

# Exemplu 
v = model_gen_reteta(wordSequencesPadded[:1])
rv = np.reshape(v, (1, 2))
print(f'Reshape id for next call: {rv}')
v = model_gen_reteta(rv)
v

start_reteta = [wordIndex['rețetă'], 0]
start_reteta_id = decodeIds(start_reteta)
start_reteta_id

start = time.time()
result = [start_reteta_id]

for n in range(250):
  nw = np.reshape(next_word_id, (1, 2))
  next_word_id = model_gen_reteta(nw)
  cuvant = decodeIds(next_word_id)
  if start_reteta_id == cuvant:
    cuvant = "\n\n" + cuvant
    cuvant += "\n"
  result.append(cuvant)

end = time.time()
print(" ".join(result))
print('\nRun time:', end - start)

"""# Poftă bună"""

print("delicios! ")